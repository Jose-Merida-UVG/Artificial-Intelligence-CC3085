{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eb7a5b",
   "metadata": {},
   "source": [
    "# Task 2 - Filtro de Spam Bayesiano\n",
    "El objetivo es construir un clasificador de texto probabilístico desde cero.  El dataset para trabajar consiste en un texto donde cada línea es ETIQUETA \\t MENSAJE, cargado desde `spam.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b4e3b",
   "metadata": {},
   "source": [
    "## 1. Pre Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3386e8",
   "metadata": {},
   "source": [
    "### Carga del Archivo\n",
    "Primero, podemos cargar el archivo a una lista de strings para verificar que se haya leído correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9c47b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/spam.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i in range(3):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135c1c5",
   "metadata": {},
   "source": [
    "### Limpieza de Texto\n",
    "Aquí aplicamos las operaciones de conversión a minúsculas, removiendo caracteres especiales y manteniendo únicamente palabras por si solas. Por decisión de diseño, estaremos removiendo completamente los números. Adicionalmente, estaremos indagando sobre caracteres especiales y que rol pueden posiblemente jugar. Por ejemplo, un signo de dolar se \"perdería\" en traducción pero podríamos encodearlo a una palabra diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51799b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 11042), (',', 1938), (\"'\", 1829), ('?', 1546), ('!', 1387), ('&', 867), (':', 738), (';', 713), ('-', 581), (')', 499), ('/', 418), ('\"', 344), ('£', 329), ('*', 311), ('#', 261), ('+', 137), ('ü', 120), ('(', 119), ('Ü', 53), ('=', 47), ('\\x92', 39), ('@', 38), ('‘', 37), ('|', 36), ('>', 31), ('$', 22), ('…', 16), ('_', 15), ('%', 10), ('–', 9), ('<', 6), ('[', 5), (']', 5), ('\\\\', 4), ('\\x94', 4), ('~', 3), ('’', 3), ('\\x96', 3), ('é', 3), ('“', 2), ('\\x91', 2), ('\\x93', 2), ('¡', 2), ('ú', 1), ('è', 1), ('^', 1), ('»', 1), ('—', 1), ('É', 1), ('ì', 1), ('鈥', 1), ('┾', 1), ('〨', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'lines' is your list of strings\n",
    "all_text = \" \".join(lines)\n",
    "symbols_only = re.findall(r'[^a-zA-Z0-9\\s]', all_text)\n",
    "print(Counter(symbols_only).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c8b06",
   "metadata": {},
   "source": [
    "De los símbolos vistos, podríamos teorizar que los siguientes pueden tener significancia: `?`, `!`, `£`, `#`, `+`, `ü`, `ú`, `é`, `É`, `@`, `$`, `%`.\n",
    "\n",
    "Adicionalmente, algunos de estos tienen pocas apariciones y podrían agruparse juntos; por ejemplo, `ü`, `ú`, `é`, `É` pueden agruparse como caracteres especiales.\n",
    "\n",
    "Vamos a definir las siguientes **\"clases\"** para el encoding:\n",
    "\n",
    "| Símbolo | Token (Clase) |\n",
    "| :--- | :--- |\n",
    "| `$`, `£`, `€` | `tagmoney` |\n",
    "| `ü`, `ú`, `é`, `É` | `tagchar` |\n",
    "| `!` | `tagexclamation` |\n",
    "| `?` | `tagquestion` |\n",
    "| `#`, `+` | `tagcontact` |\n",
    "| `%` | `tagpercent` |\n",
    "| `@` | `tagat` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2896594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tgo until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "\n",
      "ham\tok lar joking wif u oni\n",
      "\n",
      "spam\tfree entry in  tagnumber  a wkly comp to win fa cup final tkts  tagnumber st may  tagnumber  text fa to  tagnumber  to receive entry questionstd txt ratetcs apply  tagnumber over tagnumber s\n",
      "\n",
      "ham\tu dun say so early hor u c already then say\n",
      "\n",
      "ham\tnah i dont think he goes to usf he lives around here though\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "processed_lines = []\n",
    "\n",
    "# Define symbol map\n",
    "symbol_map = {\n",
    "    '$': ' tagmoney ',\n",
    "    '£': ' tagmoney ',\n",
    "    '€': ' tagmoney ',\n",
    "    'ü': ' tagchar ',\n",
    "    'ú': ' tagchar ',\n",
    "    'é': ' tagchar ',\n",
    "    'É': ' tagchar ',\n",
    "    '!': ' tagexclamation ',\n",
    "    '?': ' tagquestion ',\n",
    "    '#': ' tagcontact ',\n",
    "    '+': ' tagcontact ',\n",
    "    '%': ' tagpercent ',\n",
    "    '@': ' tagat '\n",
    "}\n",
    "\n",
    "# Clean every line read from file\n",
    "for line in lines:\n",
    "\n",
    "    # Replace numbers\n",
    "    line = re.sub(r'\\d+', ' tagnumber ', line)\n",
    "\n",
    "    # Replace symbols\n",
    "    for symbol, token in symbol_map.items():\n",
    "        line = line.replace(symbol, token)\n",
    "    \n",
    "    # Remove any non-a-z characters & lowercase everything\n",
    "    line = re.sub(r'[^a-z\\s]', '', line.lower())\n",
    "\n",
    "    # Add to list\n",
    "    processed_lines.append(line)\n",
    "\n",
    "# Print first 5\n",
    "for i in range(5):\n",
    "    print(processed_lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb9a9d",
   "metadata": {},
   "source": [
    "### 3. Generación de Vocabulario\n",
    "Aquí podemos simplemente utilizar list (set?) comprehension sobre cada línea, dónde separamos la línea por espacios en blanco ignorando el primer índice (el tag de spam o ham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8441\n"
     ]
    }
   ],
   "source": [
    "vocab = {word for line in processed_lines for word in line.split()[1:]}\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96729cb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
