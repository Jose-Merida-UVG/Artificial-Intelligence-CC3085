\documentclass[11pt]{article}
\usepackage[spanish]{babel}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}


\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    columns=flexible,
    keepspaces=true,
    commentstyle=\color{gray}\itshape,
    keywordstyle=\color{blue!70!black}\bfseries,
    stringstyle=\color{green!40!black},
    frame=L,
    xleftmargin=2em,
    showstringspaces=false,
    tabsize=4
}
\title{\textbf{Laboratorio 1: Preparación de Datos, KNN y Bias-Variance} \vspace{1em}\\
Inteligencia Artificial - CC3085}
\author{José Antonio Mérida Castejón}
\date{\today}

\begin{document}
\maketitle

\section*{Task 1}
\textit{Responda las siguientes preguntas justificando su respuesta basándose en el material visto
en clase. Se espera que demuestren análisis del caso y no solamente un ``copy-paste``.}

\subsection*{El Problema de la Convexidad (Regresión Logística)}
\textit{Durante la clase, se habló que no podemos usar el Error Cuadrático Medio (MSE) para la
Regresión Logística. Con esto en mente considere el escenario donde usted asume el rol de Lead AI
Engineer y un Junior le presenta un modelo de clasificación binaria que usa una función sigmoide, pero
insiste en entrenarlo usando la fórmula MSE porque ``es lo que usó en regresión lineal y funcionaba
    bien``.}\\

\textit{Explíquele técnicamente (y gráficamente si es necesario) por qué su modelo probablemente
se quedará atascado y no encontrará la solución óptima. Mencione el concepto de ``mínimos locales``
vs ``mínimo global``.}\\

Al entrenar modelos de machine learning, las funciones de pérdida que no tienen \textit{solución de forma 
cerrada} se optimizan por medio de \textit{descenso de gradiente}. Recordemos que la condición para tener
una \textit{solución de forma cerrada} implica poder aislar nuestros pesos ($w$) utilizando un número
finito de operaciones algebráicas. Es decir, podemos simplemente ``despejar`` para cada $w$ y encontrar
los valores de cada peso. En este caso, podemos intentar encontrar los \textit{puntos críticos} de la
función pérdida planteada por medio de la \textit{gradiente}:
$$\sum (y_i - \sigma(w^T x_i)) \sigma'(w^T x_i) x_i = 0$$

\noindent Observamos que los pesos $w$ están atrapados dentro de la función sigmoide:
$$\sigma(w^T x_i) = \frac{1}{1 + e^{-w^T x_i}}$$

\noindent Esta es una ecuación trascendental. Debido a que $w$ aparece en el exponente dentro de una sumatoria,
no existe una operación algebraica finita que permita ``despejar`` o aislar los pesos. Por lo tanto, debemos
utilizar un método numérico iterativo como el \textit{descenso de gradiente}. Adicionalmente, aunque existiera
una solución de forma cerrada tendríamos problemas al no tener una función \textit{convexa}. Recordemos
que una función convexa es aquella dónde para cualesquiera dos puntos $x_1$ y $x_2$ en su dominio, al
trazar una línea recta entre estos puntos la función queda por debajo.

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.8]
        \begin{axis}[
            axis lines = middle,
            title = {Función Convexa ($x^2$)},
            xtick = \empty, ytick = \empty,
            xmin=-2.5, xmax=2.5, ymin=-0.5, ymax=6,
            clip=false,
            declare function={f(\x) = \x^2;}
        ]
            \addplot [domain=-2.2:2.2, samples=100, thick, blue] {f(x)};
            
            \fill[black] (axis cs:0, 0) circle (2pt);
            \node[pin=270:{\small \textbf{Global}}] at (axis cs:0, 0) {};
            
        \end{axis}
        \end{tikzpicture}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.8]
        \begin{axis}[
            axis lines = middle,
            title = {Función No Convexa (MSE)},
            xtick = \empty, ytick = \empty,
            xmin=-2.5, xmax=2.5, ymin=-2, ymax=6,
            clip=false,
            declare function={g(\x) = \x^2 + 1.5*sin(deg(4*\x)) + 1;}
        ]
            \addplot [domain=-2.2:2.2, samples=200, thick, blue] {g(x)};
            
            % Precise Critical Points (Minima only)
            \fill[black] (axis cs:-0.362, {g(-0.362)}) circle (2pt);
            \node[pin=270:{\small \textbf{Global}}] at (axis cs:-0.362, {g(-0.362)}) {};
            
            \fill[black] (axis cs:1.086, {g(1.086)}) circle (2pt);
            \node[pin=270:{\small Local}] at (axis cs:1.086, {g(1.086)}) {};
            
        \end{axis}
        \end{tikzpicture}
    \end{minipage}
    \caption{Comparativa de mínimos en funciones convexas y no convexas.}
\end{figure}

Luego de la \textit{demostración por dibujo}, podemos ver que una función convexa nos es sumamente útil
al buscar optimizar nuestro modelo. Evitando formalidades y demostraciones matemáticas, tener una función
convexa nos garantiza que el mínimo encontrado sea el mínimo global. Es decir, no tenemos ``trampas`` en dónde
podamos caer al momento de buscar \textit{puntos críticos} al tener solución de forma cerrada o utilizar
    \textit{descenso de gradiente.}

    Entonces, si fuésemos a utilizar MSE como función de pérdida para este modelo obtenemos una función ``fea`` dónde
    tenemos múltiples mínimos locales y puntos silla dónde se puede quedar \textit{atascado} nuestro descenso de
    gradiente.

    Recordemos que para el descensod e gradiente, iniciamos con pesos aleatorios y damos una cierta cantidad de
    \textit{pasos} buscando la dirección opuesta a la dirección más \textit{cuesta arriba} para buscar un mínimo.
    En este caso, es muy posible que estemos dando pasos para bajar y nos quedemos ``atascados`` sin dirección
    alguna porque ya no tenemos más inclinación hacia algún punto en específico.

\subsection*{El problema de la K en KNN}

\textit{Refierase a las slides correspondientes al tema de KNN. Con esto, considere el escenario donde estamos
clasificando enfermedades raras. Tenemos un dataset de 1,000 pacientes sanos (Clase A) y solo 10
pacientes con la enfermedad (Clase B). Responda:}

\textit{Si usted elige un K=15 (un número mayor a la cantidad total de casos positivos), ¿qué es lo más
probable que ocurra matemáticamente con todas las predicciones para nuevos pacientes enfermos?
¿Por qué el algoritmo de "votación mayoritaria" falla aquí y qué técnica simple de pre\-procesamiento
(vista en el lab anterior) sería obligatoria antes de usar KNN?}\\

En este caso, lo más probable que suceda es que los pacientes enfermos serán clasificados incorrectamente
como pacientes sanos. Matemáticamente, para que un nuevo punto sea clasificado como Clase B, necesitaría
que al menos 8 de sus 15 vecinos más cercanos pertenezcan a esta clase. Dado que tenemos únicamente 10 casos
positivos en el dataset, incluso si el paciente es idéntico a los pacientes enfermos 5 de sus 15 vecinos
siguen siendo Clase A. Dado que no necesariamente todos los pacientes enfermos serán idénticos, es muy
poco probable que existan predicciones de Clase B tomando en cuenta el desbalanceo de clases.


La votación mayoritaria falla porque asume que la densidad local de los puntos es representativa de la clase,
esto causa un sesgo hacia la clase mayoritaria. En este caso, si fuésemos a aumentar aún más el valor de K
se volvería imposible que la Clase B ganara alguna votación.


La técnica de preprocesamiento a utilizar aquí sería undersampling, dónde buscaríamos tomar únicamnente 10 registros
de personas sanas de manera que queden balanceadas las clases. También existen otras técnicas cómo oversampling, SMOTE
o \textit{mandar al junior a conseguir más datos}.

\subsection*{Polinomios y la Realidad}
\textit{Observe las gráficas de las diapositivas 38 – 40. Con esto considere y responda. Si su modelo de regresión
polinomial tiene una pérdida (Loss) de casi 0.0 en el set de entrenamiento (orden 8), pero al probarlo en
producción con datos nuevos el error es gigante:}

\textit{Que nombre recibe este fenomeno?}

Este efecto recibe el nombre de overfitting, dónde el modelo empieza a memorizar ruido dentro del dataset de
entrenamiento en lugar de poder captar los patrones subyacentes. Un modelo más complejo puede ajustarse mejor
al set de entrenamiento, pero también debemos de tomar en cuenta su capacidad de generalizar. 

\textit{Que esta modelando su algoritmo que no deberia modelar}

En este caso, podemos decir que el algoritmo está modelando ruido / comportamientos únicos dentro del set de
prueba en lugar de patrones subyacentes.

\end{document}
